{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ae10cb1-6d19-47ef-98fd-0474841918ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a162365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Nodes: 373 \n",
      "# Edges: 6157\n"
     ]
    }
   ],
   "source": [
    "G = pickle.load(open('yeast_G.pickle', 'rb'))\n",
    "\n",
    "print(f'# Nodes: {G.number_of_nodes()} \\n# Edges: {G.number_of_edges()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ffdf52",
   "metadata": {},
   "source": [
    "## Add node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8dc756a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobra\n",
    "\n",
    "cobra_model = cobra.io.load_json_model('redYeast_ST8943_fdp1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24c9f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of metabolites in every reaction as a node feature\n",
    "for node, data in G.nodes(data=True):\n",
    "    if \"rev?\" in node: rxn_name = node.split(\"?\")[1]\n",
    "    else: rxn_name = node\n",
    "    \n",
    "    num_metabolites = len(cobra_model.reactions.get_by_id(rxn_name).metabolites)\n",
    "    data['x'] = num_metabolites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806c87c-843b-4eab-b8e1-ba546324007f",
   "metadata": {},
   "source": [
    "## Read ORACLE's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "44e52311-7ade-434c-bb56-66c04aa52019",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = pd.read_csv('saturations.csv', index_col=0)\n",
    "gamma = pd.read_csv('gamma.csv', index_col=0)\n",
    "vmax = pd.read_csv('Vmax_matrix.csv', index_col=0)\n",
    "\n",
    "gamma = gamma.head(1)\n",
    "sigma = sigma.head(1)\n",
    "vmax = vmax.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ffe4c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reactions that are the reversible \n",
    "rev_rxn = []\n",
    "for node in list(G.nodes()):\n",
    "    if node.split(\"?\")[0] == 'rev': rev_rxn.append(node.split(\"?\")[1])\n",
    "\n",
    "# rename the reactions of gamma; if it's the reversible one add rev? to the column name\n",
    "for col in gamma.columns:\n",
    "    if col in rev_rxn: gamma.rename(columns={col:'rev?'+col}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "42b19be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Graph but not in gamma: ['EX_lac__D_e', 'EX_mal__L_e', 'EX_akg_e', 'EX_2phetoh_e', 'EX_acald_e', 'EX_ac_e', 'EX_gam6p_e', 'EX_co2_e', 'EX_cit_e', 'EX_etoh_e', 'EX_fum_e', 'EX_gly_e', 'EX_gcald_e', 'EX_glx_e', 'EX_id3acald_e', 'EX_ala__L_e', 'EX_asn__L_e', 'EX_asp__L_e', 'EX_cys__L_e', 'EX_glu__L_e', 'EX_gln__L_e', 'EX_phe__L_e', 'EX_ser__L_e', 'EX_trp__L_e', 'EX_tyr__L_e', 'EX_oaa_e', 'EX_pacald_e', 'EX_pyr_e', 'EX_succ_e', 'EX_ind3eth_e', 'EX_h2o_e', 'EX_g6p_e', 'EX_g1p_e', 'EX_2pg_e', 'EX_pser__L_e', 'EX_ppi_e', 'EX_pep_e', 'EX_cbp_e', 'EX_6pgc_e', 'EX_3pg_e', 'EX_cmp_e', 'GROWTH', 'EX_ccm_e', 'EX_pca_e', 'rev?EX_nh4_e', 'rev?EX_glc__D_e', 'rev?EX_h_e', 'rev?EX_fe2_e', 'rev?EX_o2_e', 'rev?EX_pi_e', 'rev?EX_k_e', 'rev?EX_na1_e', 'rev?EX_so4_e', 'rev?EX_cl_e', 'rev?EX_cu2_e', 'rev?EX_mn2_e', 'rev?EX_zn2_e', 'rev?EX_mg2_e', 'rev?EX_ca2_e']\n",
      "\n",
      "In gamma but not in Graph: []\n"
     ]
    }
   ],
   "source": [
    "listA = list(G.nodes())\n",
    "listB = gamma.columns.values\n",
    "\n",
    "print('In Graph but not in gamma:', [item for item in listA if item not in listB])\n",
    "print()\n",
    "print('In gamma but not in Graph:', [item for item in listB if item not in listA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "28eb4459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Nodes: 314 \n",
      "# Edges: 6090\n"
     ]
    }
   ],
   "source": [
    "# Drop nodes without gamma\n",
    "G.remove_nodes_from([item for item in listA if item not in listB])\n",
    "print(f'# Nodes: {G.number_of_nodes()} \\n# Edges: {G.number_of_edges()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c1b46c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 243)\n",
      "# Nodes: 243 \n",
      "# Edges: 3793\n"
     ]
    }
   ],
   "source": [
    "# reactions with gamma > 1\n",
    "rxn_bad_gamma = gamma.columns[(gamma > 1).any()].tolist()\n",
    "gamma.drop(columns=rxn_bad_gamma, inplace=True)\n",
    "print(gamma.shape)\n",
    "\n",
    "# Drop nodes with gamma > 0\n",
    "G.remove_nodes_from(rxn_bad_gamma)\n",
    "print(f'# Nodes: {G.number_of_nodes()} \\n# Edges: {G.number_of_edges()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4103336f",
   "metadata": {},
   "source": [
    "#### Add `gamma` values as Graph node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f5a3dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in gamma.columns:\n",
    "    try:\n",
    "        G.nodes[node]['y'] =  gamma[node].values[0]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "no_gamma_nodes = [node for node, data in G.nodes(data=True) if not data]\n",
    "\n",
    "for node in no_gamma_nodes: G.nodes[node]['y'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5de8d226",
   "metadata": {},
   "source": [
    "#### Maybe, the Graph is ready afterall..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffaea5cd",
   "metadata": {},
   "source": [
    "## Networkx to Torch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5395fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[243, 1], edge_index=[2, 3793], y=[243], edge_attr=[3793, 1])\n",
      "\n",
      "243 3793\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = from_networkx(G, group_edge_attrs=all)\n",
    "data.x = data.x.view(-1,1).float()\n",
    "data.y = data.y.float()\n",
    "\n",
    "print(data)\n",
    "print()\n",
    "print(data.num_nodes ,data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a971ecd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[243, 1], edge_index=[2, 3793], y=[243], edge_attr=[3793, 1], train_mask=[243], val_mask=[243], test_mask=[243])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(len(data.x) * 0.8)\n",
    "val_size = int(len(data.x) * 0.1)\n",
    "test_size = int(len(data.x) * 0.1)\n",
    "\n",
    "# Create train, validation, and test masks\n",
    "train_mask = torch.zeros(len(data.x), dtype=torch.bool)\n",
    "train_mask[:train_size] = 1\n",
    "\n",
    "val_mask = torch.zeros(len(data.x), dtype=torch.bool)\n",
    "val_mask[train_size:train_size + val_size] = 1\n",
    "\n",
    "test_mask = torch.zeros(len(data.x), dtype=torch.bool)\n",
    "test_mask[train_size + val_size:] = 1\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9fb83ba",
   "metadata": {},
   "source": [
    "## Create a GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c8bbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = GCNConv(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7a55a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MAX_EPOCHS = 1000\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "INPUT_DIM = data.num_features\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = GCN(INPUT_DIM, HIDDEN_DIM)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\" ************ TRAIN FUNCTION **************\"\n",
    "def train():\n",
    "\n",
    "    model.train() # Tells the model that we are in training mode\n",
    "    optimizer.zero_grad() # Resets the gradient\n",
    "    \n",
    "    y_pred = model(data.x, data.edge_index) # predicted y\n",
    "    y_true = data.y # True labels\n",
    "    \n",
    "    loss = criterion(y_pred[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\" ************ TEST FUNCTION **************\"\n",
    "def test(mask):\n",
    "\n",
    "    model.eval() # Tells the model that we are in testing mode\n",
    "\n",
    "    y_pred = model(data.x, data.edge_index) # Preds for all data\n",
    "    y_true = data.y\n",
    "    \n",
    "    mse = criterion(y_pred[mask], y_true[mask])\n",
    "\n",
    "    return mse        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5fbae67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.3274, Val: 0.6975,         Test: 1.0574\n",
      "Epoch: 001, Loss: 0.6142, Val: 0.4700,         Test: 0.6652\n",
      "Epoch: 002, Loss: 0.3222, Val: 0.3577,         Test: 0.4285\n",
      "Epoch: 003, Loss: 0.3066, Val: 0.3601,         Test: 0.4006\n",
      "Epoch: 004, Loss: 0.4226, Val: 0.3426,         Test: 0.3902\n",
      "Epoch: 005, Loss: 0.3680, Val: 0.3352,         Test: 0.4191\n",
      "Epoch: 006, Loss: 0.2709, Val: 0.3732,         Test: 0.5198\n",
      "Epoch: 007, Loss: 0.2623, Val: 0.4219,         Test: 0.6237\n",
      "Epoch: 008, Loss: 0.3131, Val: 0.4264,         Test: 0.6403\n",
      "Epoch: 009, Loss: 0.3264, Val: 0.3829,         Test: 0.5656\n",
      "Epoch: 010, Loss: 0.2855, Val: 0.3265,         Test: 0.4597\n",
      "Epoch: 011, Loss: 0.2435, Val: 0.2876,         Test: 0.3770\n",
      "Epoch: 012, Loss: 0.2406, Val: 0.2704,         Test: 0.3336\n",
      "Epoch: 013, Loss: 0.2645, Val: 0.2622,         Test: 0.3169\n",
      "Epoch: 014, Loss: 0.2752, Val: 0.2556,         Test: 0.3166\n",
      "Epoch: 015, Loss: 0.2576, Val: 0.2552,         Test: 0.3358\n",
      "Epoch: 016, Loss: 0.2317, Val: 0.2656,         Test: 0.3748\n",
      "Epoch: 017, Loss: 0.2221, Val: 0.2814,         Test: 0.4185\n",
      "Epoch: 018, Loss: 0.2311, Val: 0.2890,         Test: 0.4415\n",
      "Epoch: 019, Loss: 0.2411, Val: 0.2800,         Test: 0.4289\n",
      "Epoch: 020, Loss: 0.2370, Val: 0.2577,         Test: 0.3869\n",
      "Epoch: 021, Loss: 0.2223, Val: 0.2330,         Test: 0.3354\n",
      "Epoch: 022, Loss: 0.2113, Val: 0.2144,         Test: 0.2926\n",
      "Epoch: 023, Loss: 0.2121, Val: 0.2035,         Test: 0.2660\n",
      "Epoch: 024, Loss: 0.2187, Val: 0.1974,         Test: 0.2547\n",
      "Epoch: 025, Loss: 0.2202, Val: 0.1946,         Test: 0.2567\n",
      "Epoch: 026, Loss: 0.2133, Val: 0.1960,         Test: 0.2711\n",
      "Epoch: 027, Loss: 0.2047, Val: 0.2016,         Test: 0.2938\n",
      "Epoch: 028, Loss: 0.2021, Val: 0.2079,         Test: 0.3147\n",
      "Epoch: 029, Loss: 0.2053, Val: 0.2092,         Test: 0.3221\n",
      "Epoch: 030, Loss: 0.2078, Val: 0.2027,         Test: 0.3106\n",
      "Epoch: 031, Loss: 0.2052, Val: 0.1907,         Test: 0.2847\n",
      "Epoch: 032, Loss: 0.1998, Val: 0.1782,         Test: 0.2551\n",
      "Epoch: 033, Loss: 0.1969, Val: 0.1689,         Test: 0.2312\n",
      "Epoch: 034, Loss: 0.1983, Val: 0.1635,         Test: 0.2174\n",
      "Epoch: 035, Loss: 0.2004, Val: 0.1611,         Test: 0.2142\n",
      "Epoch: 036, Loss: 0.1995, Val: 0.1615,         Test: 0.2206\n",
      "Epoch: 037, Loss: 0.1963, Val: 0.1644,         Test: 0.2338\n",
      "Epoch: 038, Loss: 0.1943, Val: 0.1684,         Test: 0.2479\n",
      "Epoch: 039, Loss: 0.1951, Val: 0.1703,         Test: 0.2554\n",
      "Epoch: 040, Loss: 0.1966, Val: 0.1681,         Test: 0.2515\n",
      "Epoch: 041, Loss: 0.1962, Val: 0.1624,         Test: 0.2376\n",
      "Epoch: 042, Loss: 0.1943, Val: 0.1556,         Test: 0.2197\n",
      "Epoch: 043, Loss: 0.1932, Val: 0.1503,         Test: 0.2044\n",
      "Epoch: 044, Loss: 0.1939, Val: 0.1473,         Test: 0.1958\n",
      "Epoch: 045, Loss: 0.1949, Val: 0.1465,         Test: 0.1951\n",
      "Epoch: 046, Loss: 0.1946, Val: 0.1478,         Test: 0.2012\n",
      "Epoch: 047, Loss: 0.1935, Val: 0.1505,         Test: 0.2112\n",
      "Epoch: 048, Loss: 0.1931, Val: 0.1532,         Test: 0.2201\n",
      "Epoch: 049, Loss: 0.1937, Val: 0.1541,         Test: 0.2233\n",
      "Epoch: 050, Loss: 0.1943, Val: 0.1524,         Test: 0.2189\n",
      "Epoch: 051, Loss: 0.1939, Val: 0.1489,         Test: 0.2092\n",
      "Epoch: 052, Loss: 0.1933, Val: 0.1455,         Test: 0.1987\n",
      "Epoch: 053, Loss: 0.1933, Val: 0.1433,         Test: 0.1917\n",
      "Epoch: 054, Loss: 0.1938, Val: 0.1427,         Test: 0.1900\n",
      "Epoch: 055, Loss: 0.1940, Val: 0.1437,         Test: 0.1936\n",
      "Epoch: 056, Loss: 0.1936, Val: 0.1457,         Test: 0.2006\n",
      "Epoch: 057, Loss: 0.1933, Val: 0.1480,         Test: 0.2077\n",
      "Epoch: 058, Loss: 0.1935, Val: 0.1492,         Test: 0.2114\n",
      "Epoch: 059, Loss: 0.1938, Val: 0.1487,         Test: 0.2101\n",
      "Epoch: 060, Loss: 0.1936, Val: 0.1470,         Test: 0.2048\n",
      "Epoch: 061, Loss: 0.1934, Val: 0.1451,         Test: 0.1986\n",
      "Epoch: 062, Loss: 0.1933, Val: 0.1439,         Test: 0.1945\n",
      "Epoch: 063, Loss: 0.1935, Val: 0.1438,         Test: 0.1940\n",
      "Epoch: 064, Loss: 0.1935, Val: 0.1449,         Test: 0.1973\n",
      "Epoch: 065, Loss: 0.1934, Val: 0.1466,         Test: 0.2026\n",
      "Epoch: 066, Loss: 0.1932, Val: 0.1483,         Test: 0.2076\n",
      "Epoch: 067, Loss: 0.1933, Val: 0.1492,         Test: 0.2101\n",
      "Epoch: 068, Loss: 0.1934, Val: 0.1489,         Test: 0.2092\n",
      "Epoch: 069, Loss: 0.1933, Val: 0.1480,         Test: 0.2059\n",
      "Epoch: 070, Loss: 0.1932, Val: 0.1470,         Test: 0.2024\n",
      "Epoch: 071, Loss: 0.1932, Val: 0.1466,         Test: 0.2006\n",
      "Epoch: 072, Loss: 0.1932, Val: 0.1469,         Test: 0.2014\n",
      "Epoch: 073, Loss: 0.1932, Val: 0.1480,         Test: 0.2044\n",
      "Epoch: 074, Loss: 0.1931, Val: 0.1493,         Test: 0.2083\n",
      "Epoch: 075, Loss: 0.1931, Val: 0.1504,         Test: 0.2114\n",
      "Epoch: 076, Loss: 0.1931, Val: 0.1508,         Test: 0.2123\n",
      "Epoch: 077, Loss: 0.1931, Val: 0.1504,         Test: 0.2110\n",
      "Epoch: 078, Loss: 0.1931, Val: 0.1498,         Test: 0.2087\n",
      "Epoch: 079, Loss: 0.1931, Val: 0.1493,         Test: 0.2069\n",
      "Epoch: 080, Loss: 0.1931, Val: 0.1493,         Test: 0.2065\n",
      "Epoch: 081, Loss: 0.1931, Val: 0.1498,         Test: 0.2079\n",
      "Epoch: 082, Loss: 0.1931, Val: 0.1507,         Test: 0.2104\n",
      "Epoch: 083, Loss: 0.1931, Val: 0.1515,         Test: 0.2128\n",
      "Epoch: 084, Loss: 0.1931, Val: 0.1519,         Test: 0.2139\n",
      "Epoch: 085, Loss: 0.1931, Val: 0.1518,         Test: 0.2135\n",
      "Epoch: 086, Loss: 0.1931, Val: 0.1514,         Test: 0.2120\n",
      "Epoch: 087, Loss: 0.1931, Val: 0.1509,         Test: 0.2105\n",
      "Epoch: 088, Loss: 0.1931, Val: 0.1507,         Test: 0.2097\n",
      "Epoch: 089, Loss: 0.1931, Val: 0.1509,         Test: 0.2103\n",
      "Epoch: 090, Loss: 0.1931, Val: 0.1514,         Test: 0.2117\n",
      "Epoch: 091, Loss: 0.1931, Val: 0.1519,         Test: 0.2132\n",
      "Epoch: 092, Loss: 0.1931, Val: 0.1522,         Test: 0.2140\n",
      "Epoch: 093, Loss: 0.1931, Val: 0.1521,         Test: 0.2138\n",
      "Epoch: 094, Loss: 0.1931, Val: 0.1517,         Test: 0.2127\n",
      "Epoch: 095, Loss: 0.1931, Val: 0.1513,         Test: 0.2115\n",
      "Epoch: 096, Loss: 0.1931, Val: 0.1511,         Test: 0.2108\n",
      "Epoch: 097, Loss: 0.1931, Val: 0.1511,         Test: 0.2109\n",
      "Epoch: 098, Loss: 0.1931, Val: 0.1514,         Test: 0.2117\n",
      "Epoch: 099, Loss: 0.1931, Val: 0.1516,         Test: 0.2126\n",
      "Epoch: 100, Loss: 0.1931, Val: 0.1518,         Test: 0.2131\n",
      "Epoch: 101, Loss: 0.1931, Val: 0.1516,         Test: 0.2128\n",
      "Epoch: 102, Loss: 0.1931, Val: 0.1513,         Test: 0.2120\n",
      "Epoch: 103, Loss: 0.1931, Val: 0.1510,         Test: 0.2111\n",
      "Epoch: 104, Loss: 0.1931, Val: 0.1508,         Test: 0.2105\n",
      "Epoch: 105, Loss: 0.1931, Val: 0.1508,         Test: 0.2106\n",
      "Epoch: 106, Loss: 0.1931, Val: 0.1510,         Test: 0.2111\n",
      "Epoch: 107, Loss: 0.1931, Val: 0.1511,         Test: 0.2117\n",
      "Epoch: 108, Loss: 0.1931, Val: 0.1511,         Test: 0.2119\n",
      "Epoch: 109, Loss: 0.1931, Val: 0.1510,         Test: 0.2115\n",
      "Epoch: 110, Loss: 0.1931, Val: 0.1508,         Test: 0.2109\n",
      "Epoch: 111, Loss: 0.1931, Val: 0.1505,         Test: 0.2103\n",
      "Epoch: 112, Loss: 0.1931, Val: 0.1504,         Test: 0.2100\n",
      "Epoch: 113, Loss: 0.1931, Val: 0.1505,         Test: 0.2101\n",
      "Epoch: 114, Loss: 0.1931, Val: 0.1506,         Test: 0.2105\n",
      "Epoch: 115, Loss: 0.1931, Val: 0.1506,         Test: 0.2109\n",
      "Epoch: 116, Loss: 0.1931, Val: 0.1506,         Test: 0.2109\n",
      "Epoch: 117, Loss: 0.1931, Val: 0.1505,         Test: 0.2106\n",
      "Epoch: 118, Loss: 0.1931, Val: 0.1504,         Test: 0.2101\n",
      "Epoch: 119, Loss: 0.1931, Val: 0.1502,         Test: 0.2098\n",
      "Epoch: 120, Loss: 0.1931, Val: 0.1502,         Test: 0.2098\n",
      "Epoch: 121, Loss: 0.1931, Val: 0.1503,         Test: 0.2100\n",
      "Epoch: 122, Loss: 0.1931, Val: 0.1504,         Test: 0.2103\n",
      "Epoch: 123, Loss: 0.1931, Val: 0.1504,         Test: 0.2105\n",
      "Epoch: 124, Loss: 0.1931, Val: 0.1504,         Test: 0.2104\n",
      "Epoch: 125, Loss: 0.1931, Val: 0.1503,         Test: 0.2101\n",
      "Epoch: 126, Loss: 0.1931, Val: 0.1502,         Test: 0.2099\n",
      "Epoch: 127, Loss: 0.1931, Val: 0.1502,         Test: 0.2098\n",
      "Epoch: 128, Loss: 0.1931, Val: 0.1502,         Test: 0.2099\n",
      "Epoch: 129, Loss: 0.1931, Val: 0.1503,         Test: 0.2101\n",
      "Epoch: 130, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 131, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 132, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 133, Loss: 0.1931, Val: 0.1502,         Test: 0.2100\n",
      "Epoch: 134, Loss: 0.1931, Val: 0.1502,         Test: 0.2099\n",
      "Epoch: 135, Loss: 0.1931, Val: 0.1502,         Test: 0.2100\n",
      "Epoch: 136, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 137, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 138, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 139, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 140, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 141, Loss: 0.1931, Val: 0.1502,         Test: 0.2101\n",
      "Epoch: 142, Loss: 0.1931, Val: 0.1503,         Test: 0.2101\n",
      "Epoch: 143, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 144, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 145, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 146, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 147, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 148, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 149, Loss: 0.1931, Val: 0.1503,         Test: 0.2102\n",
      "Epoch: 150, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 151, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 152, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 153, Loss: 0.1931, Val: 0.1503,         Test: 0.2104\n",
      "Epoch: 154, Loss: 0.1931, Val: 0.1503,         Test: 0.2103\n",
      "Epoch: 155, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 156, Loss: 0.1931, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 157, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 158, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 159, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 160, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 161, Loss: 0.1931, Val: 0.1502,         Test: 0.2103\n",
      "Epoch: 162, Loss: 0.1931, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 163, Loss: 0.1931, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 164, Loss: 0.1931, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 165, Loss: 0.1930, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 166, Loss: 0.1930, Val: 0.1502,         Test: 0.2102\n",
      "Epoch: 167, Loss: 0.1930, Val: 0.1501,         Test: 0.2102\n",
      "Epoch: 168, Loss: 0.1930, Val: 0.1501,         Test: 0.2102\n",
      "Epoch: 169, Loss: 0.1930, Val: 0.1501,         Test: 0.2101\n",
      "Epoch: 170, Loss: 0.1930, Val: 0.1501,         Test: 0.2101\n",
      "Epoch: 171, Loss: 0.1930, Val: 0.1501,         Test: 0.2101\n",
      "Epoch: 172, Loss: 0.1930, Val: 0.1501,         Test: 0.2102\n",
      "Epoch: 173, Loss: 0.1930, Val: 0.1501,         Test: 0.2102\n",
      "Epoch: 174, Loss: 0.1930, Val: 0.1501,         Test: 0.2101\n",
      "Epoch: 175, Loss: 0.1930, Val: 0.1501,         Test: 0.2101\n",
      "Epoch: 176, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 177, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 178, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 179, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 180, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 181, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 182, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 183, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 184, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 185, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 186, Loss: 0.1930, Val: 0.1500,         Test: 0.2101\n",
      "Epoch: 187, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 188, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 189, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 190, Loss: 0.1930, Val: 0.1500,         Test: 0.2100\n",
      "Epoch: 191, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 192, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 193, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 194, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 195, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 196, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 197, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 198, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 199, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 200, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 201, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 202, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 203, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 204, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 205, Loss: 0.1930, Val: 0.1499,         Test: 0.2100\n",
      "Epoch: 206, Loss: 0.1930, Val: 0.1498,         Test: 0.2100\n",
      "Epoch: 207, Loss: 0.1930, Val: 0.1498,         Test: 0.2100\n",
      "Epoch: 208, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 209, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 210, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 211, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 212, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 213, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 214, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 215, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 216, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 217, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 218, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 219, Loss: 0.1930, Val: 0.1498,         Test: 0.2099\n",
      "Epoch: 220, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 221, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 222, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 223, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 224, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 225, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 226, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 227, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 228, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 229, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 230, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 231, Loss: 0.1930, Val: 0.1497,         Test: 0.2099\n",
      "Epoch: 232, Loss: 0.1930, Val: 0.1497,         Test: 0.2098\n",
      "Epoch: 233, Loss: 0.1930, Val: 0.1497,         Test: 0.2098\n",
      "Epoch: 234, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 235, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 236, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 237, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 238, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 239, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 240, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 241, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 242, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 243, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 244, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 245, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 246, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 247, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 248, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 249, Loss: 0.1930, Val: 0.1496,         Test: 0.2098\n",
      "Epoch: 250, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 251, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 252, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 253, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 254, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 255, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 256, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 257, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 258, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 259, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 260, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 261, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 262, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 263, Loss: 0.1930, Val: 0.1495,         Test: 0.2098\n",
      "Epoch: 264, Loss: 0.1930, Val: 0.1495,         Test: 0.2097\n",
      "Epoch: 265, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 266, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 267, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 268, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 269, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 270, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 271, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 272, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 273, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 274, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 275, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 276, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 277, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 278, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 279, Loss: 0.1930, Val: 0.1494,         Test: 0.2097\n",
      "Epoch: 280, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 281, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 282, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 283, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 284, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 285, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 286, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 287, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 288, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 289, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 290, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 291, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 292, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 293, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 294, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 295, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 296, Loss: 0.1930, Val: 0.1493,         Test: 0.2097\n",
      "Epoch: 297, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 298, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 299, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 300, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 301, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 302, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 303, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 304, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 305, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 306, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 307, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 308, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 309, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 310, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 311, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 312, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 313, Loss: 0.1930, Val: 0.1492,         Test: 0.2097\n",
      "Epoch: 314, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 315, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 316, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 317, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 318, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 319, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 320, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 321, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 322, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 323, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 324, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 325, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 326, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 327, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 328, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 329, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 330, Loss: 0.1930, Val: 0.1491,         Test: 0.2096\n",
      "Epoch: 331, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 332, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 333, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 334, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 335, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 336, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 337, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 338, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 339, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 340, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 341, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 342, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 343, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 344, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 345, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 346, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 347, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 348, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 349, Loss: 0.1930, Val: 0.1490,         Test: 0.2096\n",
      "Epoch: 350, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 351, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 352, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 353, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 354, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 355, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 356, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 357, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 358, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 359, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 360, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 361, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 362, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 363, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 364, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 365, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 366, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 367, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 368, Loss: 0.1930, Val: 0.1489,         Test: 0.2096\n",
      "Epoch: 369, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 370, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 371, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 372, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 373, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 374, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 375, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 376, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 377, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 378, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 379, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 380, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 381, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 382, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 383, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 384, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 385, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 386, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 387, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 388, Loss: 0.1930, Val: 0.1488,         Test: 0.2096\n",
      "Epoch: 389, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 390, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 391, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 392, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 393, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 394, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 395, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 396, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 397, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 398, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 399, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 400, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 401, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 402, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 403, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 404, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 405, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 406, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 407, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 408, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 409, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 410, Loss: 0.1930, Val: 0.1487,         Test: 0.2096\n",
      "Epoch: 411, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 412, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 413, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 414, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 415, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 416, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 417, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 418, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 419, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 420, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 421, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 422, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 423, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 424, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 425, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 426, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 427, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 428, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 429, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 430, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 431, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 432, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 433, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 434, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 435, Loss: 0.1930, Val: 0.1486,         Test: 0.2096\n",
      "Epoch: 436, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 437, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 438, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 439, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 440, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 441, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 442, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 443, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 444, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 445, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 446, Loss: 0.1930, Val: 0.1485,         Test: 0.2096\n",
      "Epoch: 447, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 448, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 449, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 450, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 451, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 452, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 453, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 454, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 455, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 456, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 457, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 458, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 459, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 460, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 461, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 462, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 463, Loss: 0.1930, Val: 0.1485,         Test: 0.2095\n",
      "Epoch: 464, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 465, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 466, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 467, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 468, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 469, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 470, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 471, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 472, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 473, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 474, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 475, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 476, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 477, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 478, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 479, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 480, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 481, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 482, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 483, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 484, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 485, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 486, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 487, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 488, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 489, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 490, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 491, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 492, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 493, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 494, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 495, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 496, Loss: 0.1930, Val: 0.1484,         Test: 0.2095\n",
      "Epoch: 497, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 498, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 499, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 500, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 501, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 502, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 503, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 504, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 505, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 506, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 507, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 508, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 509, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 510, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 511, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 512, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 513, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 514, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 515, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 516, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 517, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 518, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 519, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 520, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 521, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 522, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 523, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 524, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 525, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 526, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 527, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 528, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 529, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 530, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 531, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 532, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 533, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 534, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 535, Loss: 0.1930, Val: 0.1483,         Test: 0.2095\n",
      "Epoch: 536, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 537, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 538, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 539, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 540, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 541, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 542, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 543, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 544, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 545, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 546, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 547, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 548, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 549, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 550, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 551, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 552, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 553, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 554, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 555, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 556, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 557, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 558, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 559, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 560, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 561, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 562, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 563, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 564, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 565, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 566, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 567, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 568, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 569, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 570, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 571, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 572, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 573, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 574, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 575, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 576, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 577, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 578, Loss: 0.1930, Val: 0.1482,         Test: 0.2095\n",
      "Epoch: 579, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 580, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 581, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 582, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 583, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 584, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 585, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 586, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 587, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 588, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 589, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 590, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 591, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 592, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 593, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 594, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 595, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 596, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 597, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 598, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 599, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 600, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 601, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 602, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 603, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 604, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 605, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 606, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 607, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 608, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 609, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 610, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 611, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 612, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 613, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 614, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 615, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 616, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 617, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 618, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 619, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 620, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 621, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 622, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 623, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 624, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 625, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 626, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 627, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 628, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 629, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 630, Loss: 0.1930, Val: 0.1481,         Test: 0.2095\n",
      "Epoch: 631, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 632, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 633, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 634, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 635, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 636, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 637, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 638, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 639, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 640, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 641, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 642, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 643, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 644, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 645, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 646, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 647, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 648, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 649, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 650, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 651, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 652, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 653, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 654, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 655, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 656, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 657, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 658, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 659, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 660, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 661, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 662, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 663, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 664, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 665, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 666, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 667, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 668, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 669, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 670, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 671, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 672, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 673, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 674, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 675, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 676, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 677, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 678, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 679, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 680, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 681, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 682, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 683, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 684, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 685, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 686, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 687, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 688, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 689, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 690, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 691, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 692, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 693, Loss: 0.1930, Val: 0.1480,         Test: 0.2095\n",
      "Epoch: 694, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 695, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 696, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 697, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 698, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 699, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 700, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 701, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 702, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 703, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 704, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 705, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 706, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 707, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 708, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 709, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 710, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 711, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 712, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 713, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 714, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 715, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 716, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 717, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 718, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 719, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 720, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 721, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 722, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 723, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 724, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 725, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 726, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 727, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 728, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 729, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 730, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 731, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 732, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 733, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 734, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 735, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 736, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 737, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 738, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 739, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 740, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 741, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 742, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 743, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 744, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 745, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 746, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 747, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 748, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 749, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 750, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 751, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 752, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 753, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 754, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 755, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 756, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 757, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 758, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 759, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 760, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 761, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 762, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 763, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 764, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 765, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 766, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 767, Loss: 0.1930, Val: 0.1479,         Test: 0.2095\n",
      "Epoch: 768, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 769, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 770, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 771, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 772, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 773, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 774, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 775, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 776, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 777, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 778, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 779, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 780, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 781, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 782, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 783, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 784, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 785, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 786, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 787, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 788, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 789, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 790, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 791, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 792, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 793, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 794, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 795, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 796, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 797, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 798, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 799, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 800, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 801, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 802, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 803, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 804, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 805, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 806, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 807, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 808, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 809, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 810, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 811, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 812, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 813, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 814, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 815, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 816, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 817, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 818, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 819, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 820, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 821, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 822, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 823, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 824, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 825, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 826, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 827, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 828, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 829, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 830, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 831, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 832, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 833, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 834, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 835, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 836, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 837, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 838, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 839, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 840, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 841, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 842, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 843, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 844, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 845, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 846, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 847, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 848, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 849, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 850, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 851, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 852, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 853, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 854, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 855, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 856, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 857, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 858, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 859, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 860, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 861, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 862, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 863, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 864, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 865, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 866, Loss: 0.1930, Val: 0.1478,         Test: 0.2095\n",
      "Epoch: 867, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 868, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 869, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 870, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 871, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 872, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 873, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 874, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 875, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 876, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 877, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 878, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 879, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 880, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 881, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 882, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 883, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 884, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 885, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 886, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 887, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 888, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 889, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 890, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 891, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 892, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 893, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 894, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 895, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 896, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 897, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 898, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 899, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 900, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 901, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 902, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 903, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 904, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 905, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 906, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 907, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 908, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 909, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 910, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 911, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 912, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 913, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 914, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 915, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 916, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 917, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 918, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 919, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 920, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 921, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 922, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 923, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 924, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 925, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 926, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 927, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 928, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 929, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 930, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 931, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 932, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 933, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 934, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 935, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 936, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 937, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 938, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 939, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 940, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 941, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 942, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 943, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 944, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 945, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 946, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 947, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 948, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 949, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 950, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 951, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 952, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 953, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 954, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 955, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 956, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 957, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 958, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 959, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 960, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 961, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 962, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 963, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 964, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 965, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 966, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 967, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 968, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 969, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 970, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 971, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 972, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 973, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 974, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 975, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 976, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 977, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 978, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 979, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 980, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 981, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 982, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 983, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 984, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 985, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 986, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 987, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 988, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 989, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 990, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 991, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 992, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 993, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 994, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 995, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 996, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 997, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 998, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n",
      "Epoch: 999, Loss: 0.1930, Val: 0.1477,         Test: 0.2095\n"
     ]
    }
   ],
   "source": [
    "VAL_ACCURACY = []\n",
    "TEST_ACCURACY = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    loss = train()\n",
    "\n",
    "    val_acc = test(data.val_mask)\n",
    "    test_acc = test(data.test_mask)\n",
    "\n",
    "    VAL_ACCURACY.append(val_acc)\n",
    "    TEST_ACCURACY.append(test_acc)\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, \\\n",
    "        Test: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9736a28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5263],\n",
       "        [0.5264],\n",
       "        [0.5263],\n",
       "        [0.5255],\n",
       "        [0.5263],\n",
       "        [0.5261],\n",
       "        [0.5263],\n",
       "        [0.5261],\n",
       "        [0.5263],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5262],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5261],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264],\n",
       "        [0.5264]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(data.x, data.edge_index) # Preds for all data\n",
    "y_pred = y_pred[data.test_mask]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d92c7f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9900e-01, 9.9877e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01,\n",
       "        9.9900e-01, 7.4300e-08, 6.9395e-02, 9.9900e-01, 9.9900e-01, 7.6738e-01,\n",
       "        9.9900e-01, 9.9900e-01, 5.1849e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01,\n",
       "        9.9900e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01, 9.9900e-01,\n",
       "        9.9900e-01])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = data.y[data.test_mask]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696fccc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6c04a7e252f45ffc2f7191e380805b6a5dd99aa68793d835f69a35c16ce4a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
