{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae10cb1-6d19-47ef-98fd-0474841918ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import networkx as nx\n",
    "import cobra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "import GEMtoGRAPH as gg\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181151a3-7cc4-4572-99ea-13b668dae70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 373)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cobra.io.load_json_model('redYeast_ST8943_fdp1.json')\n",
    "S = cobra.util.array.create_stoichiometric_matrix(model, array_type='DataFrame')\n",
    "S.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf38acb2-5ee5-4661-abb7-e60f124d4be5",
   "metadata": {},
   "source": [
    "# MFG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0887caf",
   "metadata": {},
   "source": [
    "#### Load TFA fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54acba48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero flux reactions: 373\n",
      "TFA fluxes: 373\n"
     ]
    }
   ],
   "source": [
    "# load tfa fluxes and send them to graph construction functions\n",
    "tfa = pd.read_csv('fluxes_for_graph.csv', index_col=0)\n",
    "tfa = tfa.head(1)\n",
    "\n",
    "zero_flux = [col for col in tfa.columns if (tfa[col] == 0).all()]\n",
    "\n",
    "print('Zero flux reactions:',len(zero_flux))\n",
    "\n",
    "tfa.drop(columns=zero_flux, inplace=True)\n",
    "print(\"TFA fluxes:\", tfa.shape[1])\n",
    "\n",
    "# For _reverse reactions we should change the sign of the flux to negative\n",
    "for col in tfa.columns:\n",
    "    if '_reverse' in col: tfa[col] = -tfa[col]\n",
    "\n",
    "\n",
    "tfa.rename(columns={col: col.split(\"_reverse_\")[0] for col in tfa.columns}, inplace=True)\n",
    "\n",
    "tfa_flux = tfa.iloc[0].values\n",
    "tfa_flux = pd.DataFrame(columns=['fluxes'], data=tfa_flux)\n",
    "tfa_flux.index = S.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61920d61",
   "metadata": {},
   "source": [
    "### Create MFG Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8996a0-d873-404b-aa6a-112d49eb93d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nodes: 746 \n",
      "# edges: 6157\n",
      "\n",
      "Removing isolated nodes...\n",
      "# nodes: 373 \n",
      "# edges: 6157\n"
     ]
    }
   ],
   "source": [
    "M, S_2m, G = gg.MFG(S, model, tfa_flux)\n",
    "\n",
    "# Remove isolated nodes from G\n",
    "print()\n",
    "print('Removing isolated nodes...')\n",
    "isolated_nodes = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolated_nodes)\n",
    "\n",
    "print(\"# nodes:\", G.number_of_nodes(), \"\\n# edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806c87c-843b-4eab-b8e1-ba546324007f",
   "metadata": {},
   "source": [
    "## Read ORACLE's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44e52311-7ade-434c-bb56-66c04aa52019",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = pd.read_csv('saturations.csv', index_col=0)\n",
    "gamma = pd.read_csv('gamma.csv', index_col=0)\n",
    "vmax = pd.read_csv('Vmax_matrix.csv', index_col=0)\n",
    "\n",
    "gamma = gamma.head(1)\n",
    "sigma = sigma.head(1)\n",
    "vmax = vmax.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe4c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reactions that are the reversible \n",
    "rev_rxn = []\n",
    "for node in list(G.nodes()):\n",
    "    if node.split(\"?\")[0] == 'rev': rev_rxn.append(node.split(\"?\")[1])\n",
    "\n",
    "# rename the reactions of gamma; if it's the reversible one add rev? to the column name\n",
    "for col in gamma.columns:\n",
    "    if col in rev_rxn: gamma.rename(columns={col:'rev?'+col}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b19be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Graph but not in gamma: ['EX_lac__D_e', 'EX_mal__L_e', 'EX_akg_e', 'EX_2phetoh_e', 'EX_acald_e', 'EX_ac_e', 'EX_gam6p_e', 'EX_co2_e', 'EX_cit_e', 'EX_etoh_e', 'EX_fum_e', 'EX_gly_e', 'EX_gcald_e', 'EX_glx_e', 'EX_id3acald_e', 'EX_ala__L_e', 'EX_asn__L_e', 'EX_asp__L_e', 'EX_cys__L_e', 'EX_glu__L_e', 'EX_gln__L_e', 'EX_phe__L_e', 'EX_ser__L_e', 'EX_trp__L_e', 'EX_tyr__L_e', 'EX_oaa_e', 'EX_pacald_e', 'EX_pyr_e', 'EX_succ_e', 'EX_ind3eth_e', 'EX_h2o_e', 'EX_g6p_e', 'EX_g1p_e', 'EX_2pg_e', 'EX_pser__L_e', 'EX_ppi_e', 'EX_pep_e', 'EX_cbp_e', 'EX_6pgc_e', 'EX_3pg_e', 'EX_cmp_e', 'GROWTH', 'EX_ccm_e', 'EX_pca_e', 'rev?EX_nh4_e', 'rev?EX_glc__D_e', 'rev?EX_h_e', 'rev?EX_fe2_e', 'rev?EX_o2_e', 'rev?EX_pi_e', 'rev?EX_k_e', 'rev?EX_na1_e', 'rev?EX_so4_e', 'rev?EX_cl_e', 'rev?EX_cu2_e', 'rev?EX_mn2_e', 'rev?EX_zn2_e', 'rev?EX_mg2_e', 'rev?EX_ca2_e']\n",
      "\n",
      "In gamma but not in Graph: []\n"
     ]
    }
   ],
   "source": [
    "listA = list(G.nodes())\n",
    "listB = gamma.columns\n",
    "\n",
    "print('In Graph but not in gamma:', [item for item in listA if item not in listB])\n",
    "print()\n",
    "print('In gamma but not in Graph:', [item for item in listB if item not in listA])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4103336f",
   "metadata": {},
   "source": [
    "#### Add `gamma` values as Graph node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a3dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in gamma.columns:\n",
    "    try:\n",
    "        G.nodes[node]['gamma'] =  gamma[node].values[0]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "no_gamma_nodes = [node for node, data in G.nodes(data=True) if not data]\n",
    "\n",
    "for node in no_gamma_nodes: G.nodes[node]['gamma'] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8e9bb6",
   "metadata": {},
   "source": [
    "### We have the `Networkx` Graph G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1120803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 6157)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eabd7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clustering</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>closeness</th>\n",
       "      <th>betweeness</th>\n",
       "      <th>pr</th>\n",
       "      <th>gamma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2OBUTtm</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.00538</td>\n",
       "      <td>0.31560</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>0.99900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2OXOADPTm</th>\n",
       "      <td>0.04000</td>\n",
       "      <td>0.03495</td>\n",
       "      <td>0.01344</td>\n",
       "      <td>0.04839</td>\n",
       "      <td>0.33442</td>\n",
       "      <td>0.00165</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>0.99900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2PHETOHtm</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.00538</td>\n",
       "      <td>0.25402</td>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00102</td>\n",
       "      <td>0.99322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6PHOPHO</th>\n",
       "      <td>0.14286</td>\n",
       "      <td>0.00538</td>\n",
       "      <td>0.01075</td>\n",
       "      <td>0.01613</td>\n",
       "      <td>0.22767</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.99900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AATA</th>\n",
       "      <td>0.10780</td>\n",
       "      <td>0.05645</td>\n",
       "      <td>0.02957</td>\n",
       "      <td>0.08602</td>\n",
       "      <td>0.31280</td>\n",
       "      <td>0.00129</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>0.30862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           clustering  in_degree  out_degree  degree_centrality  closeness  \\\n",
       "2OBUTtm       0.00000    0.00269     0.00269            0.00538    0.31560   \n",
       "2OXOADPTm     0.04000    0.03495     0.01344            0.04839    0.33442   \n",
       "2PHETOHtm     0.00000    0.00269     0.00269            0.00538    0.25402   \n",
       "6PHOPHO       0.14286    0.00538     0.01075            0.01613    0.22767   \n",
       "AATA          0.10780    0.05645     0.02957            0.08602    0.31280   \n",
       "\n",
       "           betweeness      pr   gamma  \n",
       "2OBUTtm       0.00000 0.00060 0.99900  \n",
       "2OXOADPTm     0.00165 0.00079 0.99900  \n",
       "2PHETOHtm     0.00019 0.00102 0.99322  \n",
       "6PHOPHO       0.00002 0.00080 0.99900  \n",
       "AATA          0.00129 0.00106 0.30862  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create features based on graph statistics such as degree_centrality etc\n",
    "\n",
    "df_g = pd.DataFrame(index=G.nodes())\n",
    "df_g['clustering'] = pd.Series(nx.clustering(G))\n",
    "df_g['in_degree'] = pd.Series(nx.in_degree_centrality(G))\n",
    "df_g['out_degree'] = pd.Series(nx.out_degree_centrality(G))\n",
    "df_g['degree_centrality'] = pd.Series(nx.degree_centrality(G))\n",
    "df_g['closeness'] = pd.Series(nx.closeness_centrality(G))\n",
    "df_g['betweeness'] = pd.Series(nx.betweenness_centrality(G))\n",
    "df_g['pr'] = pd.Series(nx.pagerank(G))\n",
    "\n",
    "node_data = pd.DataFrame.from_dict(dict(G.nodes), orient='index')\n",
    "node_data.sort_index(inplace=True)\n",
    "\n",
    "df_g.sort_index(inplace=True)\n",
    "\n",
    "df_g['gamma'] = node_data['gamma'].copy()\n",
    "df_g.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75f785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_g.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16418e6d",
   "metadata": {},
   "source": [
    "### Drop lines (reactions) with `gamma` > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b3fb17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these lines because of gamma > 1\n",
    "index_to_drop = df_g[df_g['gamma'] > 1].index\n",
    "\n",
    "df_g.drop(index=index_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8ad26",
   "metadata": {},
   "source": [
    "### Add more node features using data from GEM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23323d41",
   "metadata": {},
   "source": [
    "### Create a _node features_ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30792ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compartements</th>\n",
       "      <th>metabolites</th>\n",
       "      <th>num_of_mets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D_LACDcm</th>\n",
       "      <td>m|c</td>\n",
       "      <td>ficytc_m|focytc_m|lac__D_c|pyr_c</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D_LACDm</th>\n",
       "      <td>m</td>\n",
       "      <td>ficytc_m|focytc_m|lac__D_m|pyr_m</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L_LACD2cm</th>\n",
       "      <td>m|c</td>\n",
       "      <td>ficytc_m|focytc_m|lac__L_c|pyr_c</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          compartements                       metabolites num_of_mets\n",
       "D_LACDcm            m|c  ficytc_m|focytc_m|lac__D_c|pyr_c           4\n",
       "D_LACDm               m  ficytc_m|focytc_m|lac__D_m|pyr_m           4\n",
       "L_LACD2cm           m|c  ficytc_m|focytc_m|lac__L_c|pyr_c           4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(index=list(G.nodes()), columns=['compartements', 'metabolites', 'num_of_mets'])\n",
    "\n",
    "for rxn in df.index:\n",
    "    if 'rev?' in rxn: rxn = rxn.split(\"?\")[1]\n",
    "    else: rxn = rxn\n",
    "\n",
    "    metabolites = []\n",
    "    for m in model.reactions.get_by_id(rxn).metabolites: metabolites.append(m.id)\n",
    "  \n",
    "    metabolites = \"|\".join(metabolites)\n",
    "    compartements = \"|\".join(list(model.reactions.get_by_id(rxn).compartments))\n",
    "\n",
    "    # compartements = list(model.reactions.get_by_id(rxn).compartments)\n",
    "    \n",
    "    num_of_mets = len(model.reactions.get_by_id(rxn).metabolites)\n",
    "    new_row = [compartements, metabolites, num_of_mets]\n",
    "\n",
    "    df.loc[rxn] = new_row\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76fa6d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL ENCODING to every compartement and metabolite\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def enc_for_every():\n",
    "    COBRA_METABOLITES = pd.DataFrame([m.id for m in model.metabolites])\n",
    "    COBRA_METABOLITES.rename(columns = {0:'id'}, inplace = True)\n",
    "    COBRA_METABOLITES['enc'] = LabelEncoder().fit_transform(COBRA_METABOLITES['id'])\n",
    "\n",
    "    COBRA_COMPARTEMETNS = pd.DataFrame(list(model.compartments.keys()))\n",
    "    COBRA_COMPARTEMETNS.rename(columns= {0:'id'}, inplace=True)\n",
    "    COBRA_COMPARTEMETNS['enc'] = LabelEncoder().fit_transform(COBRA_COMPARTEMETNS['id'])\n",
    "\n",
    "    for row in range(len(df)):\n",
    "\n",
    "        c = df.iloc[row]['compartements']\n",
    "        m = df.iloc[row]['metabolites']\n",
    "\n",
    "        try:\n",
    "            df.loc[df.index[row], 'compartements'] = ([dict(COBRA_COMPARTEMETNS.values).get(item, item) for item in c])\n",
    "            df.loc[df.index[row], 'metabolites'] = ([dict(COBRA_METABOLITES.values).get(item, item) for item in m])\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "# Encoding for c|m -> 0, ... \n",
    "df['compartements'] = LabelEncoder().fit_transform(df['compartements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a7c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'gamma' feature name to 'y'\n",
    "for node, data in G.nodes(data=True):\n",
    "\n",
    "    data['y'] = data.pop('gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "101f3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in list(G.nodes()):\n",
    "    if 'rev?' in node: rxn = node.split(\"?\")[1]\n",
    "    else: rxn = node\n",
    "\n",
    "    # nx.set_node_attributes(G, {node: {'compartements':df.loc[rxn]['compartements']}})\n",
    "    nx.set_node_attributes(G, {node: {'num_of_mets':df.loc[rxn]['num_of_mets']}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffaea5cd",
   "metadata": {},
   "source": [
    "## Networkx to Torch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5395fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 6157], y=[373], num_of_mets=[373], edge_attr=[6157, 1], num_nodes=373)\n",
      "\n",
      "373 6157\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data = from_networkx(G, group_edge_attrs=all)\n",
    "\n",
    "print(data)\n",
    "print()\n",
    "print(data.num_nodes ,data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d647a242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 6157], y=[373], edge_attr=[6157, 1], num_nodes=373, x=[373])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x = data.num_of_mets\n",
    "del data.num_of_mets\n",
    "\n",
    "data.x = torch.tensor(data.x, dtype=torch.float)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "108d7727",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "\n",
    "class GNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='mean')\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin(x)\n",
    "        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n",
    "\n",
    "    def message(self, x_j, edge_index, size):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gnn = GNN(1, 128)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gnn(x, edge_index)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "# Define the model, loss function and optimizer\n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for data in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        labels = data.y\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch {} loss: {:.4f}\".format(epoch + 1, running_loss / len(train_dataloader)))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for data in test_dataloader:\n",
    "        outputs = model(data)\n",
    "        labels = data.y\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "    print(\"Test loss: {:.4f}\".format(running_loss / len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da049a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5d933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdfb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e1498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aae69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd12e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fe3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bb75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad8f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b01c41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GCNConv(1, 64)\n",
      "  (conv1): GCNConv(64, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  12737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "embedding_size = 64\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9526df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "data_size = len(data.x)\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "loader = DataLoader(data.x[:int(data_size * 0.8)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(data.x[int(data_size * 0.8):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ad86959",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m data_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m     15\u001b[0m NUM_GRAPHS_PER_BATCH \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m---> 16\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(data[:\u001b[39mint\u001b[39;49m(data_size \u001b[39m*\u001b[39;49m \u001b[39m0.8\u001b[39;49m)], \n\u001b[1;32m     17\u001b[0m                     batch_size\u001b[39m=\u001b[39mNUM_GRAPHS_PER_BATCH, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(data[\u001b[39mint\u001b[39m(data_size \u001b[39m*\u001b[39m \u001b[39m0.8\u001b[39m):], \n\u001b[1;32m     19\u001b[0m                          batch_size\u001b[39m=\u001b[39mNUM_GRAPHS_PER_BATCH, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(data):\n\u001b[1;32m     22\u001b[0m     \u001b[39m# Enumerate over the data\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/python-envs/main-bio/lib/python3.9/site-packages/torch_geometric/data/data.py:444\u001b[0m, in \u001b[0;36mData.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store[key]\n",
      "File \u001b[0;32m~/Desktop/python-envs/main-bio/lib/python3.9/site-packages/torch_geometric/data/storage.py:85\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Wrap data in a data loader\n",
    "data_size = len(data)\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "loader = DataLoader(data[:int(data_size * 0.8)], \n",
    "                    batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "test_loader = DataLoader(data[int(data_size * 0.8):], \n",
    "                         batch_size=NUM_GRAPHS_PER_BATCH, shuffle=True)\n",
    "\n",
    "def train(data):\n",
    "    # Enumerate over the data\n",
    "    for batch in loader:\n",
    "      # Use GPU\n",
    "      batch.to(device)  \n",
    "      # Reset gradients\n",
    "      optimizer.zero_grad() \n",
    "      # Passing the node features and the connection info\n",
    "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
    "      # Calculating the loss and gradients\n",
    "      loss = loss_fn(pred, batch.y)     \n",
    "      loss.backward()  \n",
    "      # Update using the gradients\n",
    "      optimizer.step()   \n",
    "    return loss, embedding\n",
    "\n",
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "for epoch in range(2000):\n",
    "    loss, h = train(data)\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "      print(f\"Epoch {epoch} | Train Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0f88ac7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'batch_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1001\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     33\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[56], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \n\u001b[1;32m     11\u001b[0m \u001b[39m# Use all data as input, because all nodes have node features\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)  \n\u001b[1;32m     13\u001b[0m \u001b[39m# Only use nodes with labels available for loss calculation --> mask\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[data\u001b[39m.\u001b[39mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])  \n",
      "File \u001b[0;32m~/Desktop/python-envs/main-bio/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'batch_index'"
     ]
    }
   ],
   "source": [
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "decay = 5e-4\n",
    "# Define loss function (CrossEntropyLoss for Classification Problems with \n",
    "# probability distributions)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      # Use all data as input, because all nodes have node features\n",
    "      out = model(data.x, data.edge_index)  \n",
    "      # Only use nodes with labels available for loss calculation --> mask\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  \n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      # Use the class with highest probability.\n",
    "      pred = out.argmax(dim=1)  \n",
    "      # Check against ground-truth labels.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  \n",
    "      # Derive ratio of correct predictions.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      return test_acc\n",
    "\n",
    "losses = []\n",
    "for epoch in range(0, 1001):\n",
    "    loss = train()\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc03697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(data.num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "model = GCN()\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root mean squared error\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)  \n",
    "\n",
    "# Use GPU for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_size = int(0.8 * len(data))\n",
    "test_size = len(data) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader objects for the training and test sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(data):\n",
    "    # Enumerate over the data\n",
    "    for batch in loader:\n",
    "      # Use GPU\n",
    "      batch.to(device)  \n",
    "      # Reset gradients\n",
    "      optimizer.zero_grad() \n",
    "      # Passing the node features and the connection info\n",
    "      pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
    "      # Calculating the loss and gradients\n",
    "      loss = loss_fn(pred, batch.y)     \n",
    "      loss.backward()  \n",
    "      # Update using the gradients\n",
    "      optimizer.step()   \n",
    "    return loss, embedding\n",
    "\n",
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "for epoch in range(2000):\n",
    "    loss, h = train(data)\n",
    "    losses.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "      print(f\"Epoch {epoch} | Train Loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430f39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75012619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae3548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979020e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c973772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaba04f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\" **************** CONSTRUCT THE MODEL  ********************\"\n",
    "SGC_model = SGConv(in_channels= data.num_features, # Number of features\n",
    "                   out_channels= 1, # Dimension of embedding\n",
    "                   K = 1)\n",
    "\n",
    "\" **************** GET EMBEDDING  ********************\"\n",
    "print(\" Shape of the original data: \", data.x.shape)\n",
    "print(\" Shape of the embedding data: \", SGC_model(data.x,data.edge_index).shape)\n",
    "\n",
    "\n",
    "\" **************** CONSTRUCT THE MODEL FOR REGRESSION  ********************\"\n",
    "class SGCNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SGConv(in_channels= data.num_features, # Number of features\n",
    "                   out_channels= 1, # Dimension of embedding\n",
    "                   K = 1)\n",
    " \n",
    "    def forward(self):\n",
    "        x = self.conv1(data.x,  data.edge_index) #Applying convolution to data\n",
    "        return x\n",
    "    \n",
    "\n",
    "SGC_model, data = SGCNet().to(device), data.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(SGC_model.parameters(), lr=0.2, weight_decay=0.005)\n",
    "\n",
    "# What are the learning parameters:\n",
    "for i, parameter in SGC_model.named_parameters():\n",
    "    print(\" Parameter {}\".format(i))\n",
    "    print(\"Shape: \",parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2902168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" **************** TRAIN FUNCTION ********************\"\n",
    "def train():\n",
    "    SGC_model.train() # Set the model.training to be True\n",
    "    optimizer.zero_grad() # Reset the gradient\n",
    "    predicted_y = SGC_model() # predicted y in log softmax prob\n",
    "    true_y = data.y # True labels\n",
    "    losses = F.mse_loss(predicted_y[data.train_mask], true_y[data.train_mask])\n",
    "    losses.backward()\n",
    "    optimizer.step() # Update the parameters such that is minimized the losses\n",
    "    \n",
    "    \n",
    "\" **************** TEST FUNCTION ********************\"\n",
    "def test():\n",
    "    SGC_model.eval() # Set the model.training to be False\n",
    "    logits = SGC_model() # Log prob of all data\n",
    "    accs = []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1] #Transforming log prob to actual labels\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "\n",
    "\" **************** PUTTING IT ALL TOGETHER ********************\"\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 101):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fb165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6c04a7e252f45ffc2f7191e380805b6a5dd99aa68793d835f69a35c16ce4a30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
